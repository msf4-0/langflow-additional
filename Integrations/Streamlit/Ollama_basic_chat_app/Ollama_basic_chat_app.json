{"id":"30ebbfe8-1b84-4fcf-8292-a406daeffa75","data":{"nodes":[{"id":"ConversationChain-eszTl","type":"genericNode","position":{"x":658.1167650158887,"y":561.6668320780032},"data":{"type":"ConversationChain","node":{"template":{"llm":{"type":"BaseLanguageModel","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"llm","display_name":"LLM","advanced":false,"dynamic":false,"info":"","title_case":true},"memory":{"type":"BaseMemory","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"memory","display_name":"Memory","advanced":false,"dynamic":false,"info":"Memory to load context from. If none is provided, a ConversationBufferMemory will be used.","title_case":true},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow import CustomComponent\nfrom langchain.chains import ConversationChain\nfrom typing import Optional, Union, Callable\nfrom langflow.field_typing import BaseLanguageModel, BaseMemory, Chain\n\n\nclass ConversationChainComponent(CustomComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n\n    def build_config(self):\n        return {\n            \"prompt\": {\"display_name\": \"Prompt\"},\n            \"llm\": {\"display_name\": \"LLM\"},\n            \"memory\": {\n                \"display_name\": \"Memory\",\n                \"info\": \"Memory to load context from. If none is provided, a ConversationBufferMemory will be used.\",\n            },\n            \"code\": {\"show\": False},\n        }\n\n    def build(\n        self,\n        llm: BaseLanguageModel,\n        memory: Optional[BaseMemory] = None,\n    ) -> Union[Chain, Callable]:\n        if memory is None:\n            return ConversationChain(llm=llm)\n        return ConversationChain(llm=llm, memory=memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":false,"dynamic":true,"info":"","title_case":true},"_type":"CustomComponent"},"description":"Chain to have a conversation and load context from memory.","base_classes":["Chain","Callable"],"display_name":"ConversationChain","documentation":"","custom_fields":{"llm":null,"memory":null},"output_types":["Chain","Callable"],"field_formatters":{},"beta":true},"id":"ConversationChain-eszTl"},"selected":false,"width":384,"height":397,"positionAbsolute":{"x":658.1167650158887,"y":561.6668320780032},"dragging":false},{"id":"BaseLLM-0leMO","type":"genericNode","position":{"x":33.945700112284726,"y":482.12965076033663},"data":{"type":"BaseLLM","node":{"template":{"base_url":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":true,"value":"http://192.168.0.109:11434"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List, Optional\n\nfrom langchain.llms.base import BaseLLM\nfrom langchain_community.llms.ollama import Ollama\n\nfrom langflow import CustomComponent\n\n\nclass OllamaLLM(CustomComponent):\n    display_name = \"Ollama\"\n    description = \"Local LLM with Ollama.\"\n\n    def build_config(self) -> dict:\n        return {\n            \"base_url\": {\n                \"display_name\": \"Base URL\",\n                \"info\": \"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            },\n            \"model\": {\n                \"display_name\": \"Model Name\",\n                \"value\": \"llama2\",\n                \"info\": \"Refer to https://ollama.ai/library for more models.\",\n            },\n            \"temperature\": {\n                \"display_name\": \"Temperature\",\n                \"field_type\": \"float\",\n                \"value\": 0.8,\n                \"info\": \"Controls the creativity of model responses.\",\n            },\n            \"mirostat\": {\n                \"display_name\": \"Mirostat\",\n                \"options\": [\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n                \"info\": \"Enable/disable Mirostat sampling for controlling perplexity.\",\n                \"value\": \"Disabled\",\n                \"advanced\": True,\n            },\n            \"mirostat_eta\": {\n                \"display_name\": \"Mirostat Eta\",\n                \"field_type\": \"float\",\n                \"info\": \"Learning rate influencing the algorithm's response to feedback.\",\n                \"advanced\": True,\n            },\n            \"mirostat_tau\": {\n                \"display_name\": \"Mirostat Tau\",\n                \"field_type\": \"float\",\n                \"info\": \"Controls balance between coherence and diversity.\",\n                \"advanced\": True,\n            },\n            \"num_ctx\": {\n                \"display_name\": \"Context Window Size\",\n                \"field_type\": \"int\",\n                \"info\": \"Size of the context window for generating the next token.\",\n                \"advanced\": True,\n            },\n            \"num_gpu\": {\n                \"display_name\": \"Number of GPUs\",\n                \"field_type\": \"int\",\n                \"info\": \"Number of GPUs to use for computation.\",\n                \"advanced\": True,\n            },\n            \"num_thread\": {\n                \"display_name\": \"Number of Threads\",\n                \"field_type\": \"int\",\n                \"info\": \"Number of threads to use during computation.\",\n                \"advanced\": True,\n            },\n            \"repeat_last_n\": {\n                \"display_name\": \"Repeat Last N\",\n                \"field_type\": \"int\",\n                \"info\": \"Sets how far back the model looks to prevent repetition.\",\n                \"advanced\": True,\n            },\n            \"repeat_penalty\": {\n                \"display_name\": \"Repeat Penalty\",\n                \"field_type\": \"float\",\n                \"info\": \"Penalty for repetitions in generated text.\",\n                \"advanced\": True,\n            },\n            \"stop\": {\n                \"display_name\": \"Stop Tokens\",\n                \"info\": \"List of tokens to signal the model to stop generating text.\",\n                \"advanced\": True,\n            },\n            \"tfs_z\": {\n                \"display_name\": \"TFS Z\",\n                \"field_type\": \"float\",\n                \"info\": \"Tail free sampling to reduce impact of less probable tokens.\",\n                \"advanced\": True,\n            },\n            \"top_k\": {\n                \"display_name\": \"Top K\",\n                \"field_type\": \"int\",\n                \"info\": \"Limits token selection to top K for reducing nonsense generation.\",\n                \"advanced\": True,\n            },\n            \"top_p\": {\n                \"display_name\": \"Top P\",\n                \"field_type\": \"int\",\n                \"info\": \"Works with top-k to control diversity of generated text.\",\n                \"advanced\": True,\n            },\n        }\n\n    def build(\n        self,\n        base_url: Optional[str],\n        model: str,\n        temperature: Optional[float],\n        mirostat: Optional[str],\n        mirostat_eta: Optional[float] = None,\n        mirostat_tau: Optional[float] = None,\n        num_ctx: Optional[int] = None,\n        num_gpu: Optional[int] = None,\n        num_thread: Optional[int] = None,\n        repeat_last_n: Optional[int] = None,\n        repeat_penalty: Optional[float] = None,\n        stop: Optional[List[str]] = None,\n        tfs_z: Optional[float] = None,\n        top_k: Optional[int] = None,\n        top_p: Optional[int] = None,\n    ) -> BaseLLM:\n        if not base_url:\n            base_url = \"http://localhost:11434\"\n\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n\n        try:\n            llm = Ollama(\n                base_url=base_url,\n                model=model,\n                mirostat=mirostat_value,\n                mirostat_eta=mirostat_eta,\n                mirostat_tau=mirostat_tau,\n                num_ctx=num_ctx,\n                num_gpu=num_gpu,\n                num_thread=num_thread,\n                repeat_last_n=repeat_last_n,\n                repeat_penalty=repeat_penalty,\n                temperature=temperature,\n                stop=stop,\n                tfs_z=tfs_z,\n                top_k=top_k,\n                top_p=top_p,\n            )\n\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama.\") from e\n\n        return llm\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":false,"dynamic":true,"info":"","title_case":true},"mirostat":{"type":"str","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"value":"Disabled","fileTypes":[],"file_path":"","password":false,"options":["Disabled","Mirostat","Mirostat 2.0"],"name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","title_case":true},"mirostat_eta":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate influencing the algorithm's response to feedback.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"mirostat_tau":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls balance between coherence and diversity.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"model":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"value":"llava","fileTypes":[],"file_path":"","password":false,"name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","title_case":true},"num_ctx":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating the next token.","title_case":true},"num_gpu":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation.","title_case":true},"num_thread":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation.","title_case":true},"repeat_last_n":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"Sets how far back the model looks to prevent repetition.","title_case":true},"repeat_penalty":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"stop":{"type":"str","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"stop","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"List of tokens to signal the model to stop generating text.","title_case":true},"temperature":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":0.8,"fileTypes":[],"file_path":"","password":false,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"tfs_z":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling to reduce impact of less probable tokens.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"top_k":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K for reducing nonsense generation.","title_case":true},"top_p":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works with top-k to control diversity of generated text.","title_case":true},"_type":"CustomComponent"},"description":"Local LLM with Ollama.","base_classes":["BaseLanguageModel","BaseLLM"],"display_name":"Ollama","documentation":"","custom_fields":{"base_url":null,"model":null,"temperature":null,"mirostat":null,"mirostat_eta":null,"mirostat_tau":null,"num_ctx":null,"num_gpu":null,"num_thread":null,"repeat_last_n":null,"repeat_penalty":null,"stop":null,"tfs_z":null,"top_k":null,"top_p":null},"output_types":["BaseLLM"],"field_formatters":{},"beta":true},"id":"BaseLLM-0leMO"},"selected":true,"width":384,"height":555,"positionAbsolute":{"x":33.945700112284726,"y":482.12965076033663},"dragging":false}],"edges":[{"source":"BaseLLM-0leMO","sourceHandle":"{œbaseClassesœ:[œBaseLanguageModelœ,œBaseLLMœ],œdataTypeœ:œBaseLLMœ,œidœ:œBaseLLM-0leMOœ}","target":"ConversationChain-eszTl","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œConversationChain-eszTlœ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}","data":{"targetHandle":{"fieldName":"llm","id":"ConversationChain-eszTl","inputTypes":null,"type":"BaseLanguageModel"},"sourceHandle":{"baseClasses":["BaseLanguageModel","BaseLLM"],"dataType":"BaseLLM","id":"BaseLLM-0leMO"}},"style":{"stroke":"#555"},"className":"stroke-foreground  stroke-connection","animated":false,"id":"reactflow__edge-BaseLLM-0leMO{œbaseClassesœ:[œBaseLanguageModelœ,œBaseLLMœ],œdataTypeœ:œBaseLLMœ,œidœ:œBaseLLM-0leMOœ}-ConversationChain-eszTl{œfieldNameœ:œllmœ,œidœ:œConversationChain-eszTlœ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}"}],"viewport":{"x":229.60419008255798,"y":-263.08694409821845,"zoom":0.6597539553864472}},"description":"Conversational Cartography Unlocked.","name":"Ollama_basic_chat_app","last_tested_version":"0.6.10","is_component":false}