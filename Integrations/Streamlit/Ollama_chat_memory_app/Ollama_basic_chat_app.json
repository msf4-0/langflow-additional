{"id":"30ebbfe8-1b84-4fcf-8292-a406daeffa75","data":{"nodes":[{"id":"BaseLLM-0leMO","type":"genericNode","position":{"x":12.725668181139099,"y":486.6768004598679},"data":{"type":"BaseLLM","node":{"template":{"base_url":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":true,"value":"http://192.168.0.109:11434"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List, Optional\n\nfrom langchain.llms.base import BaseLLM\nfrom langchain_community.llms.ollama import Ollama\n\nfrom langflow import CustomComponent\n\n\nclass OllamaLLM(CustomComponent):\n    display_name = \"Ollama\"\n    description = \"Local LLM with Ollama.\"\n\n    def build_config(self) -> dict:\n        return {\n            \"base_url\": {\n                \"display_name\": \"Base URL\",\n                \"info\": \"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            },\n            \"model\": {\n                \"display_name\": \"Model Name\",\n                \"value\": \"llama2\",\n                \"info\": \"Refer to https://ollama.ai/library for more models.\",\n            },\n            \"temperature\": {\n                \"display_name\": \"Temperature\",\n                \"field_type\": \"float\",\n                \"value\": 0.8,\n                \"info\": \"Controls the creativity of model responses.\",\n            },\n            \"mirostat\": {\n                \"display_name\": \"Mirostat\",\n                \"options\": [\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n                \"info\": \"Enable/disable Mirostat sampling for controlling perplexity.\",\n                \"value\": \"Disabled\",\n                \"advanced\": True,\n            },\n            \"mirostat_eta\": {\n                \"display_name\": \"Mirostat Eta\",\n                \"field_type\": \"float\",\n                \"info\": \"Learning rate influencing the algorithm's response to feedback.\",\n                \"advanced\": True,\n            },\n            \"mirostat_tau\": {\n                \"display_name\": \"Mirostat Tau\",\n                \"field_type\": \"float\",\n                \"info\": \"Controls balance between coherence and diversity.\",\n                \"advanced\": True,\n            },\n            \"num_ctx\": {\n                \"display_name\": \"Context Window Size\",\n                \"field_type\": \"int\",\n                \"info\": \"Size of the context window for generating the next token.\",\n                \"advanced\": True,\n            },\n            \"num_gpu\": {\n                \"display_name\": \"Number of GPUs\",\n                \"field_type\": \"int\",\n                \"info\": \"Number of GPUs to use for computation.\",\n                \"advanced\": True,\n            },\n            \"num_thread\": {\n                \"display_name\": \"Number of Threads\",\n                \"field_type\": \"int\",\n                \"info\": \"Number of threads to use during computation.\",\n                \"advanced\": True,\n            },\n            \"repeat_last_n\": {\n                \"display_name\": \"Repeat Last N\",\n                \"field_type\": \"int\",\n                \"info\": \"Sets how far back the model looks to prevent repetition.\",\n                \"advanced\": True,\n            },\n            \"repeat_penalty\": {\n                \"display_name\": \"Repeat Penalty\",\n                \"field_type\": \"float\",\n                \"info\": \"Penalty for repetitions in generated text.\",\n                \"advanced\": True,\n            },\n            \"stop\": {\n                \"display_name\": \"Stop Tokens\",\n                \"info\": \"List of tokens to signal the model to stop generating text.\",\n                \"advanced\": True,\n            },\n            \"tfs_z\": {\n                \"display_name\": \"TFS Z\",\n                \"field_type\": \"float\",\n                \"info\": \"Tail free sampling to reduce impact of less probable tokens.\",\n                \"advanced\": True,\n            },\n            \"top_k\": {\n                \"display_name\": \"Top K\",\n                \"field_type\": \"int\",\n                \"info\": \"Limits token selection to top K for reducing nonsense generation.\",\n                \"advanced\": True,\n            },\n            \"top_p\": {\n                \"display_name\": \"Top P\",\n                \"field_type\": \"int\",\n                \"info\": \"Works with top-k to control diversity of generated text.\",\n                \"advanced\": True,\n            },\n        }\n\n    def build(\n        self,\n        base_url: Optional[str],\n        model: str,\n        temperature: Optional[float],\n        mirostat: Optional[str],\n        mirostat_eta: Optional[float] = None,\n        mirostat_tau: Optional[float] = None,\n        num_ctx: Optional[int] = None,\n        num_gpu: Optional[int] = None,\n        num_thread: Optional[int] = None,\n        repeat_last_n: Optional[int] = None,\n        repeat_penalty: Optional[float] = None,\n        stop: Optional[List[str]] = None,\n        tfs_z: Optional[float] = None,\n        top_k: Optional[int] = None,\n        top_p: Optional[int] = None,\n    ) -> BaseLLM:\n        if not base_url:\n            base_url = \"http://localhost:11434\"\n\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n\n        try:\n            llm = Ollama(\n                base_url=base_url,\n                model=model,\n                mirostat=mirostat_value,\n                mirostat_eta=mirostat_eta,\n                mirostat_tau=mirostat_tau,\n                num_ctx=num_ctx,\n                num_gpu=num_gpu,\n                num_thread=num_thread,\n                repeat_last_n=repeat_last_n,\n                repeat_penalty=repeat_penalty,\n                temperature=temperature,\n                stop=stop,\n                tfs_z=tfs_z,\n                top_k=top_k,\n                top_p=top_p,\n            )\n\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama.\") from e\n\n        return llm\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":false,"dynamic":true,"info":"","title_case":true},"mirostat":{"type":"str","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"value":"Disabled","fileTypes":[],"file_path":"","password":false,"options":["Disabled","Mirostat","Mirostat 2.0"],"name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","title_case":true},"mirostat_eta":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate influencing the algorithm's response to feedback.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"mirostat_tau":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls balance between coherence and diversity.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"model":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"value":"llava","fileTypes":[],"file_path":"","password":false,"name":"model","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","title_case":true},"num_ctx":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating the next token.","title_case":true},"num_gpu":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation.","title_case":true},"num_thread":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation.","title_case":true},"repeat_last_n":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"Sets how far back the model looks to prevent repetition.","title_case":true},"repeat_penalty":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"stop":{"type":"str","required":false,"placeholder":"","list":true,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"stop","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"List of tokens to signal the model to stop generating text.","title_case":true},"temperature":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":0.8,"fileTypes":[],"file_path":"","password":false,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"tfs_z":{"type":"float","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling to reduce impact of less probable tokens.","rangeSpec":{"min":-1,"max":1,"step":0.1},"title_case":true},"top_k":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K for reducing nonsense generation.","title_case":true},"top_p":{"type":"int","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works with top-k to control diversity of generated text.","title_case":true},"_type":"CustomComponent"},"description":"Local LLM with Ollama.","base_classes":["BaseLanguageModel","BaseLLM"],"display_name":"Ollama","documentation":"","custom_fields":{"base_url":null,"model":null,"temperature":null,"mirostat":null,"mirostat_eta":null,"mirostat_tau":null,"num_ctx":null,"num_gpu":null,"num_thread":null,"repeat_last_n":null,"repeat_penalty":null,"stop":null,"tfs_z":null,"top_k":null,"top_p":null},"output_types":["BaseLLM"],"field_formatters":{},"beta":true},"id":"BaseLLM-0leMO"},"selected":false,"width":384,"height":555,"positionAbsolute":{"x":12.725668181139099,"y":486.6768004598679},"dragging":false},{"id":"LLMChain-TCKQ1","type":"genericNode","position":{"x":549.28933272582,"y":782.2415309293953},"data":{"type":"LLMChain","node":{"template":{"llm":{"type":"BaseLanguageModel","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"llm","display_name":"LLM","advanced":false,"dynamic":false,"info":"","title_case":true},"memory":{"type":"BaseMemory","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"memory","display_name":"Memory","advanced":false,"dynamic":false,"info":"","title_case":true},"prompt":{"type":"BasePromptTemplate","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"prompt","display_name":"Prompt","advanced":false,"dynamic":false,"info":"","title_case":true},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Callable, Optional, Union\n\nfrom langchain.chains import LLMChain\n\nfrom langflow import CustomComponent\nfrom langflow.field_typing import (\n    BaseLanguageModel,\n    BaseMemory,\n    BasePromptTemplate,\n    Chain,\n)\n\n\nclass LLMChainComponent(CustomComponent):\n    display_name = \"LLMChain\"\n    description = \"Chain to run queries against LLMs\"\n\n    def build_config(self):\n        return {\n            \"prompt\": {\"display_name\": \"Prompt\"},\n            \"llm\": {\"display_name\": \"LLM\"},\n            \"memory\": {\"display_name\": \"Memory\"},\n            \"code\": {\"show\": False},\n        }\n\n    def build(\n        self,\n        prompt: BasePromptTemplate,\n        llm: BaseLanguageModel,\n        memory: Optional[BaseMemory] = None,\n    ) -> Union[Chain, Callable, LLMChain]:\n        return LLMChain(prompt=prompt, llm=llm, memory=memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":false,"dynamic":true,"info":"","title_case":true},"_type":"CustomComponent"},"description":"Chain to run queries against LLMs","base_classes":["Chain","Callable","Chain","LLMChain"],"display_name":"LLMChain","documentation":"","custom_fields":{"prompt":null,"llm":null,"memory":null},"output_types":["Chain","Callable","LLMChain"],"field_formatters":{},"beta":true},"id":"LLMChain-TCKQ1"},"selected":false,"width":384,"height":425,"positionAbsolute":{"x":549.28933272582,"y":782.2415309293953},"dragging":false},{"id":"ConversationBufferMemory-Evg2Q","type":"genericNode","position":{"x":-429.8635692398972,"y":530.6325808886694},"data":{"type":"ConversationBufferMemory","node":{"template":{"chat_memory":{"type":"BaseChatMessageHistory","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"password":false,"name":"chat_memory","advanced":false,"dynamic":false,"info":"","title_case":true},"ai_prefix":{"type":"str","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"value":"AI","fileTypes":[],"password":false,"name":"ai_prefix","advanced":false,"dynamic":false,"info":"","title_case":true},"human_prefix":{"type":"str","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"value":"Human","fileTypes":[],"password":false,"name":"human_prefix","advanced":false,"dynamic":false,"info":"","title_case":true},"input_key":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":"question","fileTypes":[],"file_path":"","password":false,"name":"input_key","advanced":false,"dynamic":false,"info":"The variable to be used as Chat Input when more than one variable is available.","title_case":true},"memory_key":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":"chat_history","fileTypes":[],"password":false,"name":"memory_key","advanced":false,"dynamic":false,"info":"","title_case":true},"output_key":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":"answer","fileTypes":[],"file_path":"","password":false,"name":"output_key","advanced":false,"dynamic":false,"info":"The variable to be used as Chat Output (e.g. answer in a ConversationalRetrievalChain)","title_case":true},"return_messages":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"return_messages","advanced":false,"dynamic":false,"info":"","title_case":true,"value":true},"_type":"ConversationBufferMemory"},"description":"Buffer for storing conversation memory.","base_classes":["ConversationBufferMemory","BaseMemory","BaseChatMemory"],"display_name":"ConversationBufferMemory","documentation":"https://python.langchain.com/docs/modules/memory/how_to/buffer","custom_fields":{},"output_types":[],"field_formatters":{},"beta":false},"id":"ConversationBufferMemory-Evg2Q"},"selected":false,"width":384,"height":601,"positionAbsolute":{"x":-429.8635692398972,"y":530.6325808886694},"dragging":false},{"id":"PromptTemplate-WCy9z","type":"genericNode","position":{"x":0.5999356490558512,"y":1128.4311947203707},"data":{"type":"PromptTemplate","node":{"template":{"output_parser":{"type":"BaseOutputParser","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"output_parser","advanced":false,"dynamic":true,"info":"","title_case":true},"input_types":{"type":"dict","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"input_types","advanced":false,"dynamic":true,"info":"","title_case":true},"input_variables":{"type":"str","required":true,"placeholder":"","list":true,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"input_variables","advanced":false,"dynamic":true,"info":"","title_case":true,"value":["query"]},"metadata":{"type":"dict","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"metadata","advanced":false,"dynamic":true,"info":"","title_case":true},"name":{"type":"str","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"name","advanced":false,"dynamic":true,"info":"","title_case":true},"partial_variables":{"type":"dict","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"partial_variables","advanced":false,"dynamic":true,"info":"","title_case":true},"tags":{"type":"str","required":false,"placeholder":"","list":true,"show":false,"multiline":false,"fileTypes":[],"password":false,"name":"tags","advanced":false,"dynamic":true,"info":"","title_case":true},"template":{"type":"prompt","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"fileTypes":[],"password":false,"name":"template","advanced":false,"dynamic":true,"info":"","title_case":true,"value":"You are a helpful a assistant who answers all the user's {query}"},"template_format":{"type":"str","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"value":"f-string","fileTypes":[],"password":false,"name":"template_format","advanced":false,"dynamic":true,"info":"","title_case":true},"validate_template":{"type":"bool","required":false,"placeholder":"","list":false,"show":false,"multiline":false,"value":false,"fileTypes":[],"password":false,"name":"validate_template","advanced":false,"dynamic":true,"info":"","title_case":true},"_type":"PromptTemplate","query":{"type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"query","display_name":"query","advanced":false,"input_types":["Document","BaseOutputParser"],"dynamic":false,"info":"","title_case":true}},"description":"A prompt template for a language model.","icon":null,"base_classes":["StringPromptTemplate","PromptTemplate","BasePromptTemplate"],"name":"","display_name":"PromptTemplate","documentation":"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/","custom_fields":{"":["query"]},"output_types":[],"full_path":null,"field_formatters":{},"beta":false,"error":null},"id":"PromptTemplate-WCy9z","description":"A prompt template for a language model.","display_name":"PromptTemplate"},"selected":false,"width":384,"height":375,"positionAbsolute":{"x":0.5999356490558512,"y":1128.4311947203707},"dragging":false}],"edges":[{"source":"BaseLLM-0leMO","sourceHandle":"{œbaseClassesœ:[œBaseLanguageModelœ,œBaseLLMœ],œdataTypeœ:œBaseLLMœ,œidœ:œBaseLLM-0leMOœ}","target":"LLMChain-TCKQ1","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œLLMChain-TCKQ1œ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}","data":{"targetHandle":{"fieldName":"llm","id":"LLMChain-TCKQ1","inputTypes":null,"type":"BaseLanguageModel"},"sourceHandle":{"baseClasses":["BaseLanguageModel","BaseLLM"],"dataType":"BaseLLM","id":"BaseLLM-0leMO"}},"style":{"stroke":"#555"},"className":"stroke-foreground  stroke-connection","animated":false,"id":"reactflow__edge-BaseLLM-0leMO{œbaseClassesœ:[œBaseLanguageModelœ,œBaseLLMœ],œdataTypeœ:œBaseLLMœ,œidœ:œBaseLLM-0leMOœ}-LLMChain-TCKQ1{œfieldNameœ:œllmœ,œidœ:œLLMChain-TCKQ1œ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}"},{"source":"ConversationBufferMemory-Evg2Q","sourceHandle":"{œbaseClassesœ:[œConversationBufferMemoryœ,œBaseMemoryœ,œBaseChatMemoryœ],œdataTypeœ:œConversationBufferMemoryœ,œidœ:œConversationBufferMemory-Evg2Qœ}","target":"LLMChain-TCKQ1","targetHandle":"{œfieldNameœ:œmemoryœ,œidœ:œLLMChain-TCKQ1œ,œinputTypesœ:null,œtypeœ:œBaseMemoryœ}","data":{"targetHandle":{"fieldName":"memory","id":"LLMChain-TCKQ1","inputTypes":null,"type":"BaseMemory"},"sourceHandle":{"baseClasses":["ConversationBufferMemory","BaseMemory","BaseChatMemory"],"dataType":"ConversationBufferMemory","id":"ConversationBufferMemory-Evg2Q"}},"style":{"stroke":"#555"},"className":"stroke-foreground  stroke-connection","animated":false,"id":"reactflow__edge-ConversationBufferMemory-Evg2Q{œbaseClassesœ:[œConversationBufferMemoryœ,œBaseMemoryœ,œBaseChatMemoryœ],œdataTypeœ:œConversationBufferMemoryœ,œidœ:œConversationBufferMemory-Evg2Qœ}-LLMChain-TCKQ1{œfieldNameœ:œmemoryœ,œidœ:œLLMChain-TCKQ1œ,œinputTypesœ:null,œtypeœ:œBaseMemoryœ}"},{"source":"PromptTemplate-WCy9z","sourceHandle":"{œbaseClassesœ:[œStringPromptTemplateœ,œPromptTemplateœ,œBasePromptTemplateœ],œdataTypeœ:œPromptTemplateœ,œidœ:œPromptTemplate-WCy9zœ}","target":"LLMChain-TCKQ1","targetHandle":"{œfieldNameœ:œpromptœ,œidœ:œLLMChain-TCKQ1œ,œinputTypesœ:null,œtypeœ:œBasePromptTemplateœ}","data":{"targetHandle":{"fieldName":"prompt","id":"LLMChain-TCKQ1","inputTypes":null,"type":"BasePromptTemplate"},"sourceHandle":{"baseClasses":["StringPromptTemplate","PromptTemplate","BasePromptTemplate"],"dataType":"PromptTemplate","id":"PromptTemplate-WCy9z"}},"style":{"stroke":"#555"},"className":"stroke-foreground  stroke-connection","animated":false,"id":"reactflow__edge-PromptTemplate-WCy9z{œbaseClassesœ:[œStringPromptTemplateœ,œPromptTemplateœ,œBasePromptTemplateœ],œdataTypeœ:œPromptTemplateœ,œidœ:œPromptTemplate-WCy9zœ}-LLMChain-TCKQ1{œfieldNameœ:œpromptœ,œidœ:œLLMChain-TCKQ1œ,œinputTypesœ:null,œtypeœ:œBasePromptTemplateœ}"}],"viewport":{"x":409.9902650514674,"y":-216.5046361417571,"zoom":0.49999999999999994}},"description":"Conversational Cartography Unlocked.","name":"Ollama_basic_chat_app","last_tested_version":"0.6.10","is_component":false}